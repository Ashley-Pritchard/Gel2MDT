#!/bin/sh

#import relevant libraries
import psycopg2
import pandas as pd
import sys
import requests
import csv
import numpy as np
import re 
import datetime
import os
import fnmatch
import glob
import shutil
import lxml
from collections import defaultdict 
from pyliftover import LiftOver

#specify which genome assembly lifting from and to
lo = LiftOver('hg38', 'Hg19')

#connect to databse 
username = 
password = 

db_name = 'gel2mdt_db'

conn = psycopg2.connect(host='localhost', database=db_name, user=username, password=password)

cur = conn.cursor()

#check if csv files output from this script already exist in the current directory - move to a temp directory if so
existing_files = glob.glob('Gel2MDT_Export_*_MutationReport*.csv')
if len(existing_files) >0:
	if os.path.exists('temp'):
		for item in existing_files:
			shutil.copy(item, 'temp')
			os.remove(item)
	else:
		os.mkdir('temp')
		for item in existing_files:
			shutil.move(item, 'temp')

#check if empty_files.txt file exists in the directory - move to a temp directory if so
if os.path.exists('./empty_files.txt'):
	if os.path.exists('temp'):
		shutil.copy('./empty_files.txt', 'temp')
		os.remove('./empty_files.txt')

	else:
		os.mkdir('temp')
		shutil.move('./empty_files.txt', 'temp')

#read in the proband list csv file(s) provided, if there is more than 1 - run the most recent 
proband_file = glob.glob('proband_list_*.csv')
proband_file = max(proband_file, key=os.path.getctime)

#a directory named according to the import date will be created to hold Mutation Reports generated by this script
folder = proband_file.split('_')[2]
folder = folder.split('.')[0]

#check whether a directory named with the import date already exists and add a '.' for version control if so 
if os.path.exists(folder) == True:
	folder = folder + '.'

#read in the proband file as a pandas dataframe
df_gel = pd.read_csv(proband_file)

#assign each of the gel ids collected to a variable so a csv file can be created for each of them
for index, row in df_gel.iterrows():
	gel_id = str(row[0])	

	#name csv file based on gel_id to populate with SNP data
	csv_file = 'Gel2MDT_Export_%s_MutationReport.csv' % (gel_id,)

	#name csv file based on gel_id to populate with SV data - this need handling differently but will be appended to Mutation Report file later in the script 
	csv_file_SV = 'Gel2MDT_Export_%s_MutationReport_SV.csv' % (gel_id,)

	#populate SNP csv file from the database
	def variant_pull(gel_id):
		
		#specify headers of output csv file
		column_head = ['hg38 Reference Position', 'Gene', 'Reference Sequence', 'Alternative Sequence', 'Chr', 'Genotype', 'Genomic Coordinate', 'Alamut', 'Tier', 'Reference Genome']

		#pull relevant data out of the database for each gel id
		cur.execute('''
		SELECT DISTINCT "Variant"."position", "Gene"."hgnc_name", "Variant"."reference", "Variant"."alternate", "Variant"."chromosome", "ProbandVariant"."zygosity", "TranscriptVariant"."hgvs_g", "TranscriptVariant"."hgvs_g", "ProbandVariant"."max_tier", "ToolOrAssemblyVersion"."version_number"
		FROM "Proband"
		FULL OUTER JOIN "Family" ON "Proband"."family_id" = "Family"."id"
		FULL OUTER JOIN "InterpretationReportFamily" ON "Family"."id" = "InterpretationReportFamily"."participant_family_id"
		FULL OUTER JOIN "GELInterpretationReport" ON "InterpretationReportFamily"."id" = "GELInterpretationReport"."ir_family_id"
		FULL OUTER JOIN "ToolOrAssemblyVersion" ON "GELInterpretationReport"."assembly_id" = "ToolOrAssemblyVersion"."id"
		FULL OUTER JOIN "ProbandVariant" ON "GELInterpretationReport"."id" = "ProbandVariant"."interpretation_report_id"
		FULL OUTER JOIN "Variant" ON "ProbandVariant"."variant_id" = "Variant"."id"
		FULL OUTER JOIN "TranscriptVariant" ON "Variant"."id" = "TranscriptVariant"."variant_id"
		FULL OUTER JOIN "Transcript" ON "TranscriptVariant"."transcript_id" = "Transcript"."id"
		FULL OUTER JOIN "Gene" ON "Transcript"."gene_id" = "Gene"."id"
		WHERE "Variant"."position" IS NOT NULL AND "Proband"."gel_id" = %s
		''', (gel_id,))

		#write SNP csv file for each gel id
		rows = cur.fetchall()
		filename = 'Gel2MDT_Export_%s_MutationReport.csv' % (gel_id,)
		with open(filename, 'w') as file:
			writer = csv.writer(file, delimiter=',')
			writer.writerow(column_head)
			for row in rows:
				writer.writerow(row)
	
	#call function
	variant_pull(gel_id)

	#some gel ids return empty SNP csv files - delete files and add gel ids to a txt file for review
	def delete_csv(filename):
		df = pd.read_csv(filename)
		if df.dropna().empty:
			with open('empty_files.txt', 'a') as f:
				f.write(filename + '\n')
			os.remove(filename)

	#call function
	delete_csv(csv_file)

	#populate SV csv file from the database
	def variant_pull_sv(gel_id):
		
		#specify headers of output csv file
		column_head = ['Start', 'End', 'Gene', 'Chr', 'Tier', 'Reference Genome', 'Variant Type']

		#pull relevant data out of the database for each gel id
		cur.execute('''
		SELECT DISTINCT "SVRegion"."sv_start", "SVRegion"."sv_end", "Gene"."hgnc_name", "SVRegion"."chromosome", "ProbandSV"."max_tier", "ToolOrAssemblyVersion"."version_number", "SV"."variant_type"
		FROM "Proband"
		FULL OUTER JOIN "Family" ON "Proband"."family_id" = "Family"."id"
		FULL OUTER JOIN "InterpretationReportFamily" ON "Family"."id" = "InterpretationReportFamily"."participant_family_id"
		FULL OUTER JOIN "GELInterpretationReport" ON "InterpretationReportFamily"."id" = "GELInterpretationReport"."ir_family_id"
		FULL OUTER JOIN "ToolOrAssemblyVersion" ON "GELInterpretationReport"."assembly_id" = "ToolOrAssemblyVersion"."id"
		FULL OUTER JOIN "ProbandVariant" ON "GELInterpretationReport"."id" = "ProbandVariant"."interpretation_report_id"
		FULL OUTER JOIN "ProbandSV" ON "GELInterpretationReport"."id" = "ProbandSV"."interpretation_report_id"
		FULL OUTER JOIN "ProbandSVGene" ON "ProbandSV"."id" = "ProbandSVGene"."proband_sv_id"
		FULL OUTER JOIN "SV" ON "ProbandSV"."sv_id" = "SV"."id"
		FULL OUTER JOIN "SVRegion" ON "SV"."sv_region1_id" = "SVRegion"."id"
		FULL OUTER JOIN "Variant" ON "ProbandVariant"."variant_id" = "Variant"."id"
		FULL OUTER JOIN "TranscriptVariant" ON "Variant"."id" = "TranscriptVariant"."variant_id"
		FULL OUTER JOIN "Transcript" ON "TranscriptVariant"."transcript_id" = "Transcript"."id"
		FULL OUTER JOIN "Gene" ON "ProbandSVGene"."gene_id" = "Gene"."id"
		WHERE "Variant"."position" IS NOT NULL AND "Proband"."gel_id" = %s
		''', (gel_id,))

		#write SV csv file for each gel id
		rows = cur.fetchall()
		filename = 'Gel2MDT_Export_%s_MutationReport_SV.csv' % (gel_id,)
		with open(filename, 'w') as file:
			writer = csv.writer(file, delimiter=',')
			writer.writerow(column_head)
			for row in rows:
				writer.writerow(row)
	
	#call function
	variant_pull_sv(gel_id)

	#call function to delete empty SV files
	delete_csv(csv_file_SV)

#the empty_files.txt file now holds a list of empty SV and SNP gel ids - therefore truly empty gel ids are in twice and need the duplicate removing - those in once are not truely empty as they have one type of variant and need removing 

def emptyfiles():
	
	#check if file exists
	if os.path.exists('./empty_files.txt'):
	
		#read in file as pandas dataframe and provide a header name
		df = pd.read_csv('empty_files.txt', header = None, names = ['Empty Files'])

		#extract Gel ID from the list of files and save it in a new column
		df['gelID'] = df['Empty Files'].str.split('_', expand=True)[2]
		
		#identify those that are duplicated and remove those that are not
		df = df[df.duplicated(subset=['gelID'], keep=False)]
		
		#delete one of the duplicated files (the second which will always be the SV)
		df = df[df.duplicated(subset=['gelID'], keep='last')]

		#drop the Gel ID column
		df = df.drop(columns=['gelID'])
		
		#overwrite the file, don't keep the header or index
		df.to_csv('empty_files.txt', header=False, index=False, sep=',')

#call the function
emptyfiles()

##########################
#                        #
# First format snp files #
#                        #
##########################

#assign each of the csv files to a variable so LiftOver can be carried out for each of them
input_file_list = glob.glob('Gel2MDT_Export_*_MutationReport.csv')

#liftover of 'hg38 Reference Position'
def lift_over(input_file_list):
	
	#read in batch of csv files as pandas dataframe
	for input_file in input_file_list:
		df = pd.read_csv(input_file)

		#tool requires input in format 'chr1	112345678' so add 'chr' as string before chromosome no. in 'Chr' field
		df['Chr'] = 'chr' + df['Chr'].astype(str)

		#users want mitochondria in format 'M' not 'MT' - strip T
		df['Chr'] = df['Chr'].str.rstrip('T')
	
		#create blank field for Hg19 conversion position
		Hg19 = []
	
		#itterate over rows of dataframe 
		for index, row in df.iterrows():

			#perform liftover of build 38 coordinates to build 37 
			if row[9] == 'GRCh38' and row[4] != 'chrM':
				#use 'Chr' and 'hg38 Reference Position' as input for liftover tool 
				LiftOver_results = lo.convert_coordinate(row[4], row[0])
				#append results to Hg19
				Hg19.append(LiftOver_results)

			#do not perform liftover on coordinates already in build 37 or for mito 
			else:
				LiftOver_results = row[0]
				Hg19.append(LiftOver_results)

		#populate empty Hg19 field with LiftOver results
		df['Hg19'] = Hg19

		#overwrite csv file
		df.to_csv(input_file, sep=',')

#call function		
lift_over(input_file_list)

#liftover output is '[('chr1', 12345678, '+', 12345678910)] - need to extract the coordinate
def reformat_lift_over(input_file_list):

	#read in batch of csv files as pandas dataframe
	for input_file in input_file_list:
		df = pd.read_csv(input_file)

		#in the liftover function, pandas assigned an index - assign the index to this column again to prevent duplication
		df.set_index('Unnamed: 0', inplace=True)

		for index, row in df.iterrows():

			#reformat the output of build 38 to 37 liftover 
			if row[9] == 'GRCh38' and row[4] != 'chrM':

				#split the Hg19 field into 4 on ',' ( [('chr1' / 12345678 / '+' / 12345678910)] ) and save index 1 (zero based) as 'Reference Position' 
				df['Reference Position'] = df['Hg19'].str.split(',', n=4, expand=True)[1]

			#not necessary to reformat coordinates that were not lifted over
			else:
				df['Reference Position'] = df['hg38 Reference Position']

		#drop unecessary column
		df.drop(columns =['Hg19'], inplace=True)

		#overwrite csv file
		df.to_csv(input_file, sep=',')
		
#call function
reformat_lift_over(input_file_list)
	
#for del / dup / ins need to do a liftover of the 'Genomic Coordinate' which is in the format '1:g.12345678_12345680del' - first need to extract the coordinates
def extract_genomic_coord(input_file_list):

	#read in batch of csv files as pandas dataframe
	for input_file in input_file_list:
		df = pd.read_csv(input_file)

		#set index
		df.set_index('Unnamed: 0', inplace=True)

		#split 'Genomic Coordinate' into 2 on '.' (1:g / 12345678_12345680del) and save index 1 as 'GC' (genomic coordinate)
		df['GC'] = df['Genomic Coordinate'].str.split('.', n=2, expand=True)[1]
	
		# split remainder on '_' and all possible genomic changes into the two coordinates and save as new dataframe
		df2 = df['GC'].str.split('_|T|G|A|C|dup|del|ins|>', n=2, expand=True)

		#assign each coordinate to a field
		df['First Coordinate'] = df2[0]
		df['Second Coordinate'] = df2[1]

		#use split to remove the '.0' that is added to the second coordinate
		df['Second Coordinate'] = df['Second Coordinate'].str.split('.', n=2, expand=True)[0]

		#drop unnecessary column
		df.drop(columns = ['GC'], inplace=True)

		#overwrite csv file
		df.to_csv(input_file, sep=',')
	
#call function
extract_genomic_coord(input_file_list)

#can now do liftover of the two genomic coordinates
def lift_over_genomic_coord(input_file_list):

	#read in batch of csv files as pandas dataframe
	for input_file in input_file_list:
		df = pd.read_csv(input_file)

		#set index
		df.set_index('Unnamed: 0', inplace=True)

		#create empty fields to populate with liftover results
		First_Coordinate_LiftOver = []
		Second_Coordinate_LiftOver = []

		#itterate over rows of dataframe
		for index, row in df.iterrows():

			#perform liftover of build 38 coordinates to build 37
			if row[9] == 'GRCh38' and row[4] != 'chrM':
				#use 'Chr' and 'First Coordinate' as input for liftover tool
				liftover = lo.convert_coordinate(row[4], row[11])
				#append results
				First_Coordinate_LiftOver.append(liftover)
			
			#do not perform liftover of coordinates already in build 37
			else:
				#set liftover result as original 37 coordinate 
				liftover = row[11]
				#append results to Hg19
				First_Coordinate_LiftOver.append(liftover)
	
		#itterate over rows of dataframe
		for index, row in df.iterrows():

			#perform liftover of build 38 coordinates to build 37
			if row[9] == 'GRCh38' and row[4] != 'chrM':
				#use 'Chr' and 'First Coordinate' as input for liftover tool
				liftover = lo.convert_coordinate(row[4], row[12])
				#append results
				Second_Coordinate_LiftOver.append(liftover)
			
			#do not perform liftover of coordinates already in build 37
			else:
				#set liftover result as original 37 coordinate 
				liftover = row[12]
				#append results to Hg19
				Second_Coordinate_LiftOver.append(liftover)
				
		#populate empty Coordinate LiftOver fields with results
		df['First Coordinate LiftOver'] = First_Coordinate_LiftOver
		df['Second Coordinate LiftOver'] = Second_Coordinate_LiftOver

		#remove 'chr' from 'Chr' field as no longer needed for LiftOver input
		df['Chr'] = df['Chr'].str.strip('chr')

		#overwrite csv file
		df.to_csv(input_file, sep=',')

#call function
lift_over_genomic_coord(input_file_list)

#as above, liftover output is '[('chr1', 12345678, '+', 12345678910)] - need to extract the coordinate
def reformat_genomic_lift_over(input_file_list):

	#read in batch of csv files as pandas dataframe
	for input_file in input_file_list:
		df = pd.read_csv(input_file)

		#set index
		df.set_index('Unnamed: 0', inplace=True)

		for index, row in df.iterrows():

			#reformat the output of build 38 to 37 liftover	
			if row[9] == 'GRCh38' and row[4] != 'M':
				#split 'First Coordinate LiftOver' into 4 on ',' ( [('chr1' / 12345678 / '+' / 12345678910)] ) and save index 1 as 'First Referenece Position'
				df['First Reference Position'] = df['First Coordinate LiftOver'].str.split(',', n=4, expand=True)[1]
				#do the same with the 'Second Coordinate LiftOver'. Also split on '[]' as this is the output for the rows without a second coordinate to liftover. 
				df['Second Reference Position'] = df['Second Coordinate LiftOver'].str.split(',|\[|]', n=4, expand=True)[2]
				#use split to remove the '.0' that is added to the second reference position
				df['Second Reference Position'] = df['Second Reference Position'].str.split('.', n=2, expand=True)[0]

			#not necessary to reformat coordinates that were already build 37 before saving to 'Reference Position'
			else:
				df['First Reference Position'] = df['First Coordinate']
				df['Second Reference Position'] = df['Second Coordinate']

		#drop unneccasary columns 
		df.drop(columns = ['First Coordinate LiftOver', 'Second Coordinate LiftOver'], inplace=True)

		#overwrite csv file
		df.to_csv(input_file, sep=',')
	
#call function
reformat_genomic_lift_over(input_file_list)

#reformat the 'Genomic Coordinate' field to contain the LiftOver output 'Chr1.hg19:g.12345678(_12345689)
def update_genomic_coord(input_file_list):

	#read in batch of csv files as pandas dataframe
	for input_file in input_file_list:
		df = pd.read_csv(input_file)

		#set index
		df.set_index('Unnamed: 0', inplace=True)

		#missence variants do not have a 'Second Reference Position' - fill the 'nan' with empty string to allow processing 
		df['Second Reference Position'] = df['Second Reference Position'].fillna('')
		#concatinate the two reference positions with '_' between as a string
		df['Concat Reference Positions'] = df['First Reference Position'].astype(str) + '_' + df['Second Reference Position'].astype(str)
		#use split to remove the '.0' that is added to the second reference position
		df['Concat Reference Positions'] = df['Concat Reference Positions'].str.split('.', n=2, expand=True)[0]
		#strip the '_' from the missence variants which do not have a second reference position
		df['Concat Reference Positions'] = df['Concat Reference Positions'].str.rstrip('_')

		#split the alamut column into 2 on '.' and save the 0 index (chr.g) to a new column
		df2 = df['Alamut'].str.split('.', n=2, expand=True)
		df['Chrom.g'] = df2[0]

		#missence variants do not have a 'Second Reference Position' - fill the 'nan' with empty string to allow processing 
		df['Second Coordinate'] = df['Second Coordinate'].fillna('')
		#concatinate the two reference positions with '_' between as a string
		df['Concat Coordinates'] = df['First Coordinate'].astype(str) + '_' + df['Second Coordinate'].astype(str)
		#use split to remove the '.0' that is added to the second reference position
		df['Concat Coordinates'] = df['Concat Coordinates'].str.split('.', n=2, expand=True)[0]
		#strip the '_' from the missence variants which do not have a second reference position
		df['Concat Coordinates'] = df['Concat Coordinates'].str.rstrip('_')
		
		#create a field with the original coordinates in the format 1:g.12345678
		df['Delete'] = df['Chrom.g'] + '.' + df['Concat Coordinates'].astype(str)

		#replace this field from the 'Genomic Change' field to leave only the genomic change
		df['Genomic Change'] = df['Alamut'].replace(to_replace= df['Delete'], value='', regex=True)
	
		#concatinate the chromosome, '.hg19:g.' and reference position to give the 'Genomic Coordinate'
		df['Genomic Coordinate'] = 'Chr' + df['Chr'].astype(str) + '.hg19:g.' + df['Concat Reference Positions'].astype(str) + df['Genomic Change']

		#drop unnecessary columns
		df.drop(columns = ['First Reference Position', 'Second Reference Position', 'First Coordinate', 'Second Coordinate','Chrom.g', 'Concat Coordinates', 'Delete'], inplace=True)
	
		#overwrite csv file
		df.to_csv(input_file, sep=',')

#call function
update_genomic_coord(input_file_list)

#reformat the 'Alamut' field to contain the LiftOver output '1:g.12345678G>A / 1:g.12345678_12345689del
def update_alamut_coord(input_file_list):

	#read in batch of csv files as pandas dataframe
	for input_file in input_file_list:
		df = pd.read_csv(input_file)

		#set index
		df.set_index('Unnamed: 0', inplace=True)

		#reformat the 'Alamut' field to the LiftOver output 1:g.12345678A>G / 1:g.12345678_12345689del
		df['Alamut'] = df['Chr'].astype(str) + ':' + df['Concat Reference Positions'].astype(str) + df['Genomic Change'].astype(str)

		#drop unnecessary columns
		df.drop(columns = ['Concat Reference Positions', 'Genomic Change'], inplace=True)
	
		#overwrite csv file 
		df.to_csv(input_file, sep=',')

#call function
update_alamut_coord(input_file_list)

#reorder the columns to match the csv file format used downstream in the workflow
def reorder(input_file_list):
	
	#read in batch of csv files as pandas dataframe
	for input_file in input_file_list:
		df = pd.read_csv(input_file)

		#set index
		df.set_index('Unnamed: 0', inplace=True)

		#drop the original hg38 Reference Position
		df.drop(columns = ['hg38 Reference Position'], inplace=True)

		#add empty columns columns to match SV file
		df['Variant Type'] = ''

		#Reorder the columns of the dataframe 
		df = df[['Reference Position', 'Gene', 'Reference Sequence', 'Alternative Sequence', 'Chr', 'Genotype', 'Genomic Coordinate', 'Alamut', 'Tier', 'Variant Type']]

		#overwrite csv, don't save the index
		df.to_csv(input_file, sep=',', index=False)

#call function
reorder(input_file_list)

########################
#                      #
# Next format sv files #
#                      #
########################

#assign each of the SV csv files to a variable so functions can be carried out for each of them
input_file_list_SV = glob.glob('Gel2MDT_Export_*_MutationReport_SV.csv')

#the SV csv files give rise to many replica rows due to one SV crossing over numerous genes - want to make this one record with the genes listed as ; seperated values
def combine_genes(input_file_list_SV):

	#read in batch of csv files as pandas dataframe 
	for input_file in input_file_list_SV:
		df = pd.read_csv(input_file)

		#group the other identical columns and join the genes as ; seperated calues 
		df = df.groupby(['Start', 'End', 'Chr', 'Tier', 'Reference Genome', 'Variant Type']).agg(lambda x: '; '.join(set(x)))

		#overwrite the csv files
		df.to_csv(input_file, sep=',')

#call the function
combine_genes(input_file_list_SV)

#liftover of the start and end positions
def lift_over_SV(input_file_list_SV):
	
	#read in batch of csv files as pandas dataframe
	for input_file in input_file_list_SV:
		df = pd.read_csv(input_file, header=0, names= ['Start', 'End', 'Chr', 'Tier', 'Reference Genome', 'Variant Type', 'Gene'])

		#tool requires input in format 'chr1 112345678' so add 'chr' as string before chromosome no. in 'Chr' field
		df['Chr'] = 'chr' + df['Chr'].astype(str)

		#users want mitochondria in format 'M' not 'MT' - strip T
		df['Chr'] = df['Chr'].str.rstrip('T')
	
		#create blank field for Hg19 conversion position
		start_Hg19 = []
		end_Hg19 = []
				
		#itterate over rows of dataframe 
		for index, row in df.iterrows():

			#perform liftover of build 38 coordinates to build 37 
			if row[4] == 'GRCh38' and row[2] != 'chrM':
				#use 'Chr' and 'hg38 Reference Position' as input for liftover tool 
				LiftOver_results_start = lo.convert_coordinate(row[2], row[0])
				LiftOver_results_end = lo.convert_coordinate(row[2], row[1])
				#append results to Hg19
				start_Hg19.append(LiftOver_results_start)
				end_Hg19.append(LiftOver_results_end)


			#do not perform liftover of coordinates already in build 37
			else:
				#set liftover result as original 37 coordinate  
				LiftOver_results_start = row[0]
				LiftOver_results_end = row[1]

				#append results to Hg19
				start_Hg19.append(LiftOver_results_start)
				end_Hg19.append(LiftOver_results_end)


		#populate empty Hg19 field with LiftOver results
		df['start_Hg19'] = start_Hg19
		df['end_Hg19'] = end_Hg19

		#overwrite csv file
		df.to_csv(input_file, sep=',')

#call function		
lift_over_SV(input_file_list_SV)

#as above, liftover output is '[('chr1', 12345678, '+', 12345678910)] - need to extract the coordinate
def reformat_lift_over_SV(input_file_list_SV):

	#read in batch of csv files as pandas dataframe
	for input_file in input_file_list_SV:
		df = pd.read_csv(input_file)

		#in the liftover function, pandas assigned an index - assign the index to this column again to prevent duplication
		df.set_index('Unnamed: 0', inplace=True)

		for index, row in df.iterrows():

			#reformat the output of build 38 to 37 liftover 
			if row[4] == 'GRCh38' and row[2] != 'chrM':
				#split the Hg19 field into 4 on ',' ( [('chr1' / 12345678 / '+' / 12345678910)] ) and save index 1 (zero based) as 'Reference Position' 
				df['Start'] = df['start_Hg19'].str.split(',', n=4, expand=True)[1]
				df['End'] = df['end_Hg19'].str.split(',', n=4, expand=True)[1]
				df['Reference Position'] = df['Start'].str.replace(" ", "") + '-' + df['End'].str.replace(" ", "")	
			
			#not necessary to reformat coordinates that were not lifted over
			else:
				df['Reference Position'] = df['start_Hg19'].str.replace(" ","") + '-' + df['end_Hg19'].str.replace(" ", "")

		#drop unecessary column				
		df.drop(columns = ['Start', 'End', 'Reference Genome', 'start_Hg19', 'end_Hg19'], inplace=True)

		#overwrite csv file
		df.to_csv(input_file, sep=',')
		
#call function
reformat_lift_over_SV(input_file_list_SV)

#reorder the columns to match the csv file format used downstream in the workflow
def reformat_SV(input_file_list_SV):
	
	#read in batch of csv files as pandas dataframe
	for input_file in input_file_list_SV:
		df = pd.read_csv(input_file, header=0, names= ['Chr', 'Tier', 'Variant Type', 'Gene', 'Reference Position'])

		#remove 'chr' from Chr fields
		df['Chr'] = df['Chr'].str.strip('chr')

		#create Genomic Coordinate column in the format chr:start-end
		df['Genomic Coordinate'] = df['Chr'] + ':' + df['Reference Position'].str.replace(" ", "")

		#remove 'Tier' from Tier column 
		df['Tier'] = df['Tier'].str.strip('TIER')

		#add empty columns columns to match SNP file
		df['Reference Sequence'] = ''
		df['Alternative Sequence'] = ''
		df['Genotype'] = ''
		df['Alamut'] = '' 
		

		#Reorder the columns of the dataframe 
		df = df[['Reference Position', 'Gene', 'Reference Sequence', 'Alternative Sequence', 'Chr', 'Genotype', 'Genomic Coordinate', 'Alamut', 'Tier', 'Variant Type']]

		#overwrite csv, don't save the index
		df.to_csv(input_file, sep=',', index=False)

#call function
reformat_SV(input_file_list_SV)

##########################
#                        #
# Merge and format files #
#                        #
##########################

def merge():

	#create lists of the two file types
	snp_files = glob.glob('Gel2MDT_Export_*port.csv') 
	sv_files = glob.glob('Gel2MDT_Export_*port_SV.csv') 

	#if there are snp files, iterate through them
	if len(sv_files) > 0:
		if len(snp_files) > 0:
			for x in snp_files:
				#set the first 30 characters of the filename string to 'match'
				match = x[:30]
				#iterate through the sv files
				for y in sv_files:
					#if the first 30 characters match a snp file, concat the two files and overwrite the snp file
					if y[:30] == match:
						a = pd.read_csv(x)
						b = pd.read_csv(y)
						merged = pd.concat([a, b])
						merged.to_csv(x, index = None)
						#remove the sv file
						os.remove(y)

#call function
merge()

#rename remaining SV files (without matching SNP file) to mutation report convention
def rename_sv():

	#create list
	sv_files = glob.glob('Gel2MDT_Export_*port_SV.csv') 

	#iterate through sv files and remove '_SV'
	for y in sv_files:
		new_filename = y.replace('_SV', '')
		os.rename(y, new_filename)

#call function
rename_sv()

#add the date and time to the csv file for tracability
def add_date_time(input_file_list):

	#set the date variable to the current date and time 
	date = datetime.datetime.now()

	#open csv file with new empty line 
	for input_file in input_file_list:
		with open(input_file, newline='') as f:
			r = csv.reader(f)
			data = [line for line in r]
	
			#write '#Export: todays date and time' in the form '%c' - inbuilt python for local appropriate date and time representation
			with open(input_file,'w',newline='') as f:
				w = csv.writer(f)
				w.writerow(['#Export date: ' + date.strftime('%c')])
				w.writerows(data)
				f.close()

#call function
add_date_time(input_file_list)		

#move files into a relevant directory
def make_folder(input_file_list):
	
	#make directory named according to the import date or the name provided by the user
	os.mkdir(folder)

	#move generated files into the folder
	if os.path.exists('empty_files.txt'):
		shutil.move('empty_files.txt', folder)

	shutil.move(proband_file, folder)

	for item in input_file_list:
		shutil.move(item, folder)

#call function
make_folder(input_file_list)
